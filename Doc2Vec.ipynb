{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Doc2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praj9719/ref_repo/blob/main/Doc2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmYUHP8QfYeN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdc24233-5118-4def-b849-91018ad47607"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')  \n",
        "nltk.download('punkt')\n",
        "!pip install num2words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting num2words\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/a2/ea800689730732e27711c41beed4b2a129b34974435bdc450377ec407738/num2words-0.5.10-py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words) (0.6.2)\n",
            "Installing collected packages: num2words\n",
            "Successfully installed num2words-0.5.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U674NjehOhb",
        "outputId": "18f6a976-b6f0-404a-8bec-115a58394ffc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukHZGBe4g7Lq"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "from num2words import num2words\n",
        "\n",
        "import nltk\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vQZZ2pyhKed"
      },
      "source": [
        "class PreProcessor:\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.data = data;\n",
        "\n",
        "    def execute(self):\n",
        "        self.convert_lower_case()\n",
        "        self.remove_punctuation() #remove comma seperately\n",
        "        self.remove_apostrophe()\n",
        "        self.remove_stop_words()\n",
        "        self.convert_numbers()\n",
        "        self.stemming()\n",
        "        self.remove_punctuation()\n",
        "        self.convert_numbers()\n",
        "        self.stemming() #needed again as we need to stem the words\n",
        "        self.remove_punctuation() #needed again as num2word is giving few hypens and commas fourty-one\n",
        "        self.remove_stop_words() #needed again as num2word is giving stop words 101 - one hundred and one        \n",
        "        return self.data\n",
        "\n",
        "    def convert_lower_case(self):\n",
        "        self.data = np.char.lower(self.data)\n",
        "\n",
        "    def remove_stop_words(self):\n",
        "        stop_words = stopwords.words('english')\n",
        "        words = word_tokenize(str(self.data))\n",
        "        new_text = \"\"\n",
        "        for w in words:\n",
        "            if w not in stop_words and len(w) > 1:\n",
        "                new_text = new_text + \" \" + w\n",
        "        self.data = new_text\n",
        "    \n",
        "    def remove_punctuation(self):\n",
        "        symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "        for i in range(len(symbols)):\n",
        "            data = np.char.replace(self.data, symbols[i], ' ')\n",
        "            data = np.char.replace(data, \"  \", \" \")\n",
        "        data = np.char.replace(data, ',', '')\n",
        "        self.data = data\n",
        "\n",
        "    def remove_apostrophe(self):\n",
        "        self.data = np.char.replace(self.data, \"'\", \"\")\n",
        "\n",
        "    def stemming(self):\n",
        "        stemmer= PorterStemmer()\n",
        "        \n",
        "        tokens = word_tokenize(str(self.data))\n",
        "        new_text = \"\"\n",
        "        for w in tokens:\n",
        "            new_text = new_text + \" \" + stemmer.stem(w)\n",
        "        self.data = new_text\n",
        "\n",
        "    def convert_numbers(self):\n",
        "        tokens = word_tokenize(str(self.data))\n",
        "        new_text = \"\"\n",
        "        for w in tokens:\n",
        "            try:\n",
        "                w = num2words(int(w))\n",
        "            except:\n",
        "                a = 0\n",
        "            new_text = new_text + \" \" + w\n",
        "        new_text = np.char.replace(new_text, \"-\", \" \")\n",
        "        self.data = new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NPYPtOTAhhdg",
        "outputId": "2a6d0e4d-17fc-431c-df0d-8c3a0c212be8"
      },
      "source": [
        "PreProcessor(\"what is newtons second law of motion\").execute()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' newtron second law motion'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCzXq2tokeZB"
      },
      "source": [
        "class Doc2Vec:\n",
        "\n",
        "    def __init__(self, data_path):\n",
        "        self.root = data_path\n",
        "        self.topics = []\n",
        "        self.texts = []\n",
        "        self.extract()\n",
        "        self.alpha = 0.3\n",
        "        self.N = len(self.texts)\n",
        "        print(f'[Info] alpha: {self.alpha} N: {self.N}')\n",
        "\n",
        "    def extract(self):\n",
        "        print(f'[Info] root: {self.root}')\n",
        "        files = os.listdir(self.root)\n",
        "        for file in files:\n",
        "            self.topics.append(file[:-4])\n",
        "            with open(self.root + file, 'r') as f:\n",
        "                self.texts.append(f.read())\n",
        "        print(f'[Info] total topics: {len(self.topics)}')\n",
        "\n",
        "    def execute(self):\n",
        "        self.pre_process()\n",
        "        self.caculate_df()\n",
        "        self.calculate_tf_idf()\n",
        "        self.calculate_tf_idf_title()\n",
        "        for i in self.tf_idf:\n",
        "            self.tf_idf[i] *= self.alpha\n",
        "        # for i in self.tf_idf_title:\n",
        "        #     self.tf_idf[i] = self.tf_idf_title[i]\n",
        "        self.D = np.zeros((self.N, self.total_vocab_size))\n",
        "        for i in self.tf_idf:\n",
        "            try:\n",
        "                ind = self.total_vocab.index(i[1])\n",
        "                self.D[i[0]][ind] = self.tf_idf[i]\n",
        "            except:\n",
        "                pass\n",
        "        print(f'[Info] DocVector Shape: {self.D.shape}')\n",
        "        \n",
        "    def pre_process(self):\n",
        "        self.processed_text = []\n",
        "        self.processed_title = []\n",
        "        for i in range(self.N):\n",
        "            self.processed_text.append(word_tokenize(str(PreProcessor(self.texts[i]).execute())))\n",
        "            self.processed_title.append(word_tokenize(str(PreProcessor(self.topics[i]).execute())))\n",
        "        print(f'[Info] Processed Titles: {len(self.processed_title)} Processed Texts: {len(self.processed_text)}')\n",
        "\n",
        "    def caculate_df(self):\n",
        "        self.DF = {}\n",
        "        for i in range(self.N):\n",
        "            tokens = self.processed_text[i]\n",
        "            for w in tokens:\n",
        "                try:\n",
        "                    self.DF[w].add(i)\n",
        "                except:\n",
        "                    self.DF[w] = {i}\n",
        "            tokens = self.processed_title[i]\n",
        "            for w in tokens:\n",
        "                try:\n",
        "                    self.DF[w].add(i)\n",
        "                except:\n",
        "                    self.DF[w] = {i}\n",
        "        for i in self.DF:\n",
        "            self.DF[i] = len(self.DF[i])    \n",
        "\n",
        "        self.total_vocab_size = len(self.DF)\n",
        "        self.total_vocab = [x for x in self.DF]\n",
        "        print(f'[Info] Vocabulary Size: {self.total_vocab_size} Samples: {np.array(self.total_vocab[:8])}')\n",
        "\n",
        "    def doc_freq(self, word):\n",
        "        c = 0\n",
        "        try:\n",
        "            c = self.DF[word]\n",
        "        except:\n",
        "            pass\n",
        "        return c\n",
        "\n",
        "    def calculate_tf_idf(self):\n",
        "        doc = 0\n",
        "        self.tf_idf = {}\n",
        "        for i in range(self.N):\n",
        "            tokens = self.processed_text[i]\n",
        "            counter = Counter(tokens + self.processed_title[i])\n",
        "            words_count = len(tokens + self.processed_title[i])\n",
        "            for token in np.unique(tokens):\n",
        "                tf = counter[token]/words_count\n",
        "                df = self.doc_freq(token)\n",
        "                idf = np.log((self.N+1)/(df+1))\n",
        "                self.tf_idf[doc, token] = tf*idf\n",
        "            doc += 1\n",
        "    \n",
        "    def calculate_tf_idf_title(self):\n",
        "        doc = 0\n",
        "        self.tf_idf_title = {}\n",
        "        for i in range(self.N):\n",
        "            tokens = self.processed_title[i]\n",
        "            counter = Counter(tokens + self.processed_text[i])\n",
        "            words_count = len(tokens + self.processed_text[i])\n",
        "            for token in np.unique(tokens):\n",
        "                tf = counter[token]/words_count\n",
        "                df = self.doc_freq(token)\n",
        "                idf = np.log((self.N+1)/(df+1))\n",
        "                self.tf_idf_title[doc, token] = tf*idf\n",
        "            doc += 1\n",
        "\n",
        "    def cosine_sim(self, a, b):\n",
        "        cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
        "        return cos_sim\n",
        "\n",
        "    def gen_vector(self, tokens):\n",
        "        Q = np.zeros((len(self.total_vocab)))\n",
        "        counter = Counter(tokens)\n",
        "        words_count = len(tokens)\n",
        "\n",
        "        query_weights = {}\n",
        "        \n",
        "        for token in np.unique(tokens):\n",
        "            tf = counter[token]/words_count\n",
        "            df = self.doc_freq(token)\n",
        "            idf = math.log((self.N+1)/(df+1))\n",
        "            try:\n",
        "                ind = self.total_vocab.index(token)\n",
        "                Q[ind] = tf*idf\n",
        "            except:\n",
        "                pass\n",
        "        return Q\n",
        "\n",
        "    def classify(self, k, query):\n",
        "        preprocessed_query = PreProcessor(query).execute()\n",
        "        tokens = word_tokenize(str(preprocessed_query))\n",
        "\n",
        "        d_cosines = []\n",
        "        query_vector = self.gen_vector(tokens)\n",
        "\n",
        "        for d in self.D:\n",
        "            sim = self.cosine_sim(query_vector, d)\n",
        "            d_cosines.append(sim)\n",
        "\n",
        "        out = np.array(d_cosines).argsort()[-k:][::-1]\n",
        "        \n",
        "        result = []\n",
        "        for o in out:\n",
        "            result.append({self.topics[o], round(d_cosines[o]+0.0001, 4)})\n",
        "        return result\n",
        "\n",
        "    def save(self, path, name):\n",
        "        file_path = f'{path}{name}.json'\n",
        "        print(f'[Info] saving file at {file_path}')\n",
        "        data = {}\n",
        "        data[\"N\"] = self.N\n",
        "        data[\"Name\"] = name\n",
        "        data[\"topics\"] = self.topics\n",
        "        data[\"DF\"] = self.DF\n",
        "        data[\"total_vocab\"] = self.total_vocab\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(data, f)\n",
        "        print(f'[Info] saving vector...')\n",
        "        np.save(path + name, self.D)\n",
        "        print(f'[Info] files saved!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1TJSFCb1xQ5"
      },
      "source": [
        "# Science"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcwGcVB_pNDB",
        "outputId": "e49497a6-b95c-4e64-f17b-7315779b233d"
      },
      "source": [
        "science = Doc2Vec('/content/gdrive/MyDrive/ircb/datasets/base/Science/')\n",
        "science.execute()\n",
        "science.save('/content/gdrive/MyDrive/ircb/vectors/Base/', 'Science')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Info] root: /content/gdrive/MyDrive/ircb/datasets/base/Science/\n",
            "[Info] total topics: 6\n",
            "[Info] alpha: 0.3 N: 6\n",
            "[Info] Processed Titles: 6 Processed Texts: 6\n",
            "[Info] Vocabulary Size: 952 Samples: ['newton' 'law' 'motion' 'relat' 'forc' 'act' 'bodi' 'first']\n",
            "[Info] DocVector Shape: (6, 952)\n",
            "[Info] saving file at /content/gdrive/MyDrive/ircb/vectors/Base/Science.json\n",
            "[Info] saving vector...\n",
            "[Info] files saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqHMr6NJwgdj",
        "outputId": "94318bf6-a6cc-42da-ff30-dd5807092fdc"
      },
      "source": [
        "science.classify(3, \"what is diode\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{0.0786, 'semiconductor'},\n",
              " {0.0001, 'human digestive system'},\n",
              " {0.0001, 'atom'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex9Fq2-P60ug",
        "outputId": "f5c745e7-9afd-42d5-a589-c865fe4fdc70"
      },
      "source": [
        "science.classify(3, \"newtons second law of motion\")[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0.499, 'laws of motion'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fx_tIab118y"
      },
      "source": [
        "## History"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkVkp2H60s7I",
        "outputId": "463a0705-8cc9-466f-875d-90f4b3e16639"
      },
      "source": [
        "history = Doc2Vec('/content/gdrive/MyDrive/ircb/datasets/base/History/')\n",
        "history.execute()\n",
        "history.save('/content/gdrive/MyDrive/ircb/vectors/Base/', 'History')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Info] root: /content/gdrive/MyDrive/ircb/datasets/base/History/\n",
            "[Info] total topics: 10\n",
            "[Info] alpha: 0.3 N: 10\n",
            "[Info] Processed Titles: 10 Processed Texts: 10\n",
            "[Info] Vocabulary Size: 2089 Samples: ['known' 'great' 'war' '—a' 'land' 'air' 'sea' 'conflict']\n",
            "[Info] DocVector Shape: (10, 2089)\n",
            "[Info] saving file at /content/gdrive/MyDrive/ircb/vectors/Base/History.json\n",
            "[Info] saving vector...\n",
            "[Info] files saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnrFm9B_1I_b",
        "outputId": "6e767a32-8972-43eb-8c4b-4f7f6798bba0"
      },
      "source": [
        "history.classify(3, \"Who is shivaji maharaj\")[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0.1665, 'Maratha Empire'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCOsu3vN1luW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp91XeYgRJ6C"
      },
      "source": [
        "# SSC "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cp9kt2a-ccv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a65b65df-6da4-4836-ee17-a0f88ce430cc"
      },
      "source": [
        "science = Doc2Vec('/content/gdrive/MyDrive/ircb/datasets/ssc/english/science/')\n",
        "science.execute()\n",
        "science.save('/content/gdrive/MyDrive/ircb/vectors/ssc/', 'ssc_science')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Info] root: /content/gdrive/MyDrive/ircb/datasets/ssc/english/science/\n",
            "[Info] total topics: 10\n",
            "[Info] alpha: 0.3 N: 10\n",
            "[Info] Processed Titles: 10 Processed Texts: 10\n",
            "[Info] Vocabulary Size: 2949 Samples: ['asexu' 'reproduct' 'process' 'format' 'new' 'organ' 'speci' 'without']\n",
            "[Info] DocVector Shape: (10, 2949)\n",
            "[Info] saving file at /content/gdrive/MyDrive/ircb/vectors/ssc/ssc_science.json\n",
            "[Info] saving vector...\n",
            "[Info] files saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVSrucG6S5ww",
        "outputId": "f9b6cebb-1bfe-4e0a-c1a7-d72da9dcffb4"
      },
      "source": [
        "science.classify(3, \"What is microbiology\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{0.1289, 'Introduction to Microbiology'},\n",
              " {0.0001, 'Disaster Management'},\n",
              " {0.0001, 'Social Health'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjnJa7V7Rsvi",
        "outputId": "9c43b18e-552c-4b87-c389-fce3a5613afc"
      },
      "source": [
        "history = Doc2Vec('/content/gdrive/MyDrive/ircb/datasets/ssc/english/history/')\n",
        "history.execute()\n",
        "history.save('/content/gdrive/MyDrive/ircb/vectors/ssc/', 'ssc_history')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Info] root: /content/gdrive/MyDrive/ircb/datasets/ssc/english/history/\n",
            "[Info] total topics: 9\n",
            "[Info] alpha: 0.3 N: 9\n",
            "[Info] Processed Titles: 9 Processed Texts: 9\n",
            "[Info] Vocabulary Size: 2764 Samples: ['histor' 'research' 'write' 'studi' 'carri' 'object' 'understand'\n",
            " 'chronolog']\n",
            "[Info] DocVector Shape: (9, 2764)\n",
            "[Info] saving file at /content/gdrive/MyDrive/ircb/vectors/ssc/ssc_history.json\n",
            "[Info] saving vector...\n",
            "[Info] files saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy5aPG3oStdQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}